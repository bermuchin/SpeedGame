# agent.py
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import numpy as np
import collections
import config 

class MotionAgent:
    def __init__(self):
        print(">>> [Agent] A6000 Brain ì´ˆê¸°í™” ì¤‘...")
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            config.MODEL_ID,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
        self.processor = AutoProcessor.from_pretrained(config.MODEL_ID)
        self.check_health()

    def check_health(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ì§„ë‹¨"""
        print("\n" + "="*30)
        print("ğŸ©º Agent Health Check")
        if torch.cuda.is_available():
            vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"âœ… GPU: {torch.cuda.get_device_name(0)} (VRAM: {vram:.2f} GB)")
        print(f"âœ… Model Device: {self.model.device}")
        print("="*30 + "\n")

    def preprocess_image(self, image_np):
        """ì¤‘ì•™ í¬ë¡­ ë° ë¦¬ì‚¬ì´ì§• (ë°°ê²½ ì œê±° íš¨ê³¼)"""
        h, w, _ = image_np.shape
        min_dim = min(h, w)
        # Center Crop
        start_x = (w - min_dim) // 2
        start_y = (h - min_dim) // 2
        cropped = image_np[start_y:start_y+min_dim, start_x:start_x+min_dim]
        # Resize to 480x480 (Speed optimization)
        return Image.fromarray(cropped).resize((480, 480))

    def infer_stream(self, image, selected_categories, frame_buffer):
        """ìŠ¤íŠ¸ë¦¬ë° ì¶”ë¡  ë©”ì¸ ë¡œì§"""
        # 1. ì˜ˆì™¸ ì²˜ë¦¬
        if image is None:
            return "ì¹´ë©”ë¼ ëŒ€ê¸° ì¤‘...", "...", frame_buffer

        # 2. ë²„í¼ ê´€ë¦¬ (State handling)
        if frame_buffer is None:
            frame_buffer = collections.deque(maxlen=6)
        
        # ì „ì²˜ë¦¬(Crop) í›„ ë²„í¼ì— ì¶”ê°€
        processed_img = self.preprocess_image(image)
        frame_buffer.append(processed_img)

        # ë²„í¼ê°€ ì¶©ë¶„íˆ ì°¨ì§€ ì•Šì•˜ìœ¼ë©´ ëŒ€ê¸° (Latency ì¡°ì ˆ)
        if len(frame_buffer) < 3:
            return "ì •ë³´ ìˆ˜ì§‘ ì¤‘...", "...", frame_buffer

        # 3. ì¹´í…Œê³ ë¦¬ ì¤€ë¹„
        candidates = []
        if not selected_categories: selected_categories = ["ë™ë¬¼", "ê°ì •/ìƒíƒœ"]
        for cat in selected_categories:
            candidates.extend(config.WORD_DICT.get(cat, []))

        # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_text = config.SYSTEM_PROMPT_TEMPLATE.format(candidates=', '.join(candidates))
        
        # 5. ì…ë ¥ ë©”ì‹œì§€ êµ¬ì„± (Multi-frame)
        content_list = []
        # ë²„í¼ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
        for img in frame_buffer:
            content_list.append({"type": "image", "image": img})
        content_list.append({"type": "text", "text": prompt_text})

        messages = [{"role": "user", "content": content_list}]

        # 6. ì¶”ë¡  ì‹¤í–‰
        text_input = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        
        inputs = self.processor(
            text=[text_input], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt"
        ).to("cuda")

        generated_ids = self.model.generate(**inputs, max_new_tokens=512)
        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
        full_text = self.processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]

        # 7. ê²°ê³¼ íŒŒì‹±
        try:
            if "ê´€ì°°:" in full_text and "ì •ë‹µ:" in full_text:
                perception = full_text.split("ê´€ì°°:")[1].split("ì •ë‹µ:")[0].strip()
                decision = full_text.split("ì •ë‹µ:")[1].strip()
            else:
                perception = full_text
                decision = "..."
        except:
            perception = "í•´ì„ ì¤‘..."
            decision = "..."

        return perception, decision, frame_buffer
